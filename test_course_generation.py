#!/usr/bin/env python3
"""
Test script for intelligent course generation functionality
"""
import requests
import json
import time
import sys

# API base URL
BASE_URL = "http://localhost:8000/api/v1"

def test_course_generation():
    """Test the course generation API"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ™ºèƒ½è¯¾ç¨‹ç”ŸæˆåŠŸèƒ½...")
    
    # Test data
    test_data = {
        "document_ids": [],  # ç©ºæ–‡æ¡£IDåˆ—è¡¨ï¼Œå°†ä½¿ç”¨è¯¾ç¨‹æ ‡é¢˜å’Œæè¿°ç”Ÿæˆ
        "course_config": {
            "name": "Pythonç¼–ç¨‹å…¥é—¨",
            "type": "ç¼–ç¨‹æŠ€æœ¯",
            "audience": "ç¼–ç¨‹åˆå­¦è€…",
            "level": "beginner",
            "duration": "16è¯¾æ—¶",
            "chapters": 6
        }
    }
    
    try:
        # 1. å‘èµ·è¯¾ç¨‹ç”Ÿæˆè¯·æ±‚
        print("ğŸ“ å‘é€è¯¾ç¨‹ç”Ÿæˆè¯·æ±‚...")
        response = requests.post(
            f"{BASE_URL}/courses/generate",
            json=test_data,
            headers={"Content-Type": "application/json"},
            timeout=30
        )
        
        if response.status_code != 200:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            return False
        
        result = response.json()
        print(f"âœ… è¯¾ç¨‹ç”Ÿæˆè¯·æ±‚æˆåŠŸ")
        print(f"   è¯¾ç¨‹ID: {result['course_id']}")
        print(f"   ä»»åŠ¡ID: {result['task_id']}")
        
        course_id = result['course_id']
        
        # 2. ç­‰å¾…ç”Ÿæˆå®Œæˆ
        print("â³ ç­‰å¾…AIç”Ÿæˆè¯¾ç¨‹å†…å®¹...")
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©åå°ä»»åŠ¡å®Œæˆ
        for i in range(10):
            time.sleep(2)
            
            # æ£€æŸ¥è¯¾ç¨‹çŠ¶æ€
            try:
                course_response = requests.get(f"{BASE_URL}/courses/{course_id}")
                if course_response.status_code == 200:
                    print(f"   æ£€æŸ¥è¿›åº¦ ({i+1}/10)...")
                else:
                    print(f"   æ— æ³•è·å–è¯¾ç¨‹çŠ¶æ€: {course_response.status_code}")
            except:
                print(f"   ç½‘ç»œé”™è¯¯ï¼Œç»§ç»­ç­‰å¾…...")
        
        # 3. è·å–è¯¾ç¨‹åˆ—è¡¨éªŒè¯
        print("ğŸ“‹ éªŒè¯ç”Ÿæˆçš„è¯¾ç¨‹...")
        courses_response = requests.get(f"{BASE_URL}/courses/")
        
        if courses_response.status_code != 200:
            print(f"âŒ è·å–è¯¾ç¨‹åˆ—è¡¨å¤±è´¥: {courses_response.status_code}")
            return False
        
        courses = courses_response.json()
        
        # æŸ¥æ‰¾æˆ‘ä»¬ç”Ÿæˆçš„è¯¾ç¨‹
        generated_course = None
        for course in courses:
            if course['id'] == course_id:
                generated_course = course
                break
        
        if not generated_course:
            print(f"âŒ æœªæ‰¾åˆ°ç”Ÿæˆçš„è¯¾ç¨‹ (ID: {course_id})")
            return False
        
        print("âœ… æ‰¾åˆ°ç”Ÿæˆçš„è¯¾ç¨‹:")
        print(f"   æ ‡é¢˜: {generated_course['title']}")
        print(f"   æè¿°: {generated_course['description']}")
        print(f"   çŠ¶æ€: {generated_course['status']}")
        print(f"   ç« èŠ‚æ•°: {generated_course['chapters']}")
        
        # 4. æ£€æŸ¥æ˜¯å¦çœŸçš„è°ƒç”¨äº†LLM
        print("\nğŸ” æ£€æŸ¥LLMè°ƒç”¨ç»“æœ...")
        
        # æ£€æŸ¥åç«¯æ—¥å¿—
        print("ğŸ“Š æŸ¥çœ‹Dockeræ—¥å¿—...")
        import subprocess
        try:
            log_result = subprocess.run(
                ["docker-compose", "logs", "--tail=50", "backend"], 
                capture_output=True, 
                text=True,
                cwd="E:/project/CurriculumKnowledgePlanning"
            )
            
            logs = log_result.stdout
            
            # æ£€æŸ¥å…³é”®æ—¥å¿—
            llm_indicators = [
                "Starting LLM-based content generation",
                "Course introduction generated successfully",
                "Learning objectives generated successfully", 
                "Generated", "chapters with sections",
                "Course content generation completed"
            ]
            
            found_indicators = []
            for indicator in llm_indicators:
                if indicator in logs:
                    found_indicators.append(indicator)
            
            if found_indicators:
                print("âœ… å‘ç°LLMè°ƒç”¨è¯æ®:")
                for indicator in found_indicators:
                    print(f"   âœ“ {indicator}")
            else:
                print("âŒ æœªå‘ç°LLMè°ƒç”¨è¯æ®")
                print("æœ€è¿‘çš„æ—¥å¿—:")
                print(logs[-1000:])
                
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è·å–Dockeræ—¥å¿—: {e}")
        
        return True
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        return False
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return False

def test_llm_service_directly():
    """ç›´æ¥æµ‹è¯•LLMæœåŠ¡"""
    print("\nğŸ§ª ç›´æ¥æµ‹è¯•LLMæœåŠ¡...")
    
    try:
        # è¿™éœ€è¦åœ¨å®¹å™¨å†…è¿è¡Œ
        test_script = """
import sys
sys.path.append('/app')
from app.services.llm_service import LLMService

llm_service = LLMService()
result = llm_service.generate_course_introduction(
    document_content="è¿™æ˜¯ä¸€ä¸ªPythonç¼–ç¨‹å…¥é—¨è¯¾ç¨‹ï¼Œé¢å‘åˆå­¦è€…", 
    course_type="ç¼–ç¨‹æŠ€æœ¯"
)
print("LLM Service Test Result:", result)
"""
        
        import subprocess
        result = subprocess.run([
            "docker-compose", "exec", "-T", "backend", 
            "python", "-c", test_script
        ], capture_output=True, text=True, cwd="E:/project/CurriculumKnowledgePlanning")
        
        print(f"ç›´æ¥æµ‹è¯•ç»“æœ:")
        print(f"Exit code: {result.returncode}")
        print(f"Stdout: {result.stdout}")
        if result.stderr:
            print(f"Stderr: {result.stderr}")
            
    except Exception as e:
        print(f"âŒ ç›´æ¥æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    print("="*60)
    print("ğŸ“ Curriculum Knowledge Planning - æ™ºèƒ½è¯¾ç¨‹ç”Ÿæˆæµ‹è¯•")
    print("="*60)
    
    # æµ‹è¯•API
    success = test_course_generation()
    
    # ç›´æ¥æµ‹è¯•LLMæœåŠ¡
    test_llm_service_directly()
    
    if success:
        print("\nğŸ‰ æµ‹è¯•å®Œæˆ! æ™ºèƒ½è¯¾ç¨‹ç”ŸæˆåŠŸèƒ½è¿è¡Œæ­£å¸¸")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥! éœ€è¦æ£€æŸ¥é—®é¢˜")
        sys.exit(1)